# Week 4: Introduction to Neural Networks

## Learning Objectives
By the end of this week, you will be able to:
- Understand the fundamental concepts of neural networks
- Explain the role of activation functions in neural networks
- Implement a simple neural network from scratch using NumPy
- Build and train neural networks using TensorFlow/Keras and PyTorch
- Understand the backpropagation algorithm conceptually
- Compare and contrast different optimization techniques

## Weekly Schedule

### Day 1: Neural Network Fundamentals
- From Perceptrons to Multi-Layer Perceptrons (MLPs)
- Activation Functions (Sigmoid, ReLU, Tanh, Softmax)
- Forward Propagation
- Loss Functions

### Day 2: Training Neural Networks
- Backpropagation (Conceptual Understanding)
- Gradient Descent and Variants (SGD, Momentum, Adam)
- Learning Rate and Optimization
- Weight Initialization

### Day 3: Building Neural Networks with Keras
- Introduction to TensorFlow/Keras
- Sequential API
- Dense Layers and Model Architecture
- Training and Evaluation

### Day 4: Introduction to PyTorch
- PyTorch Fundamentals
- Autograd and Tensors
- Building Neural Networks in PyTorch
- Comparing Keras and PyTorch

### Day 5: Practical Applications and Best Practices
- Overfitting and Regularization
- Batch Normalization
- Dropout
- Hyperparameter Tuning

## Prerequisites
- Python programming
- NumPy and Pandas
- Basic understanding of linear algebra
- Understanding of gradient descent (from Week 3)

## Resources
- [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/)
- [Deep Learning Book (Chapters 1-6)](https://www.deeplearningbook.org/)
- [TensorFlow Tutorials](https://www.tensorflow.org/tutorials)
- [PyTorch Tutorials](https://pytorch.org/tutorials/)
