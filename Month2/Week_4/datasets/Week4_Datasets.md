# Month 2, Week 4: Datasets for Boosting & Support Vector Machines (SVM)

This week, as we delve into advanced ensemble methods like Boosting and powerful non-linear classifiers like Support Vector Machines (SVMs), we'll work with datasets that can truly showcase their strengths. These models are often chosen for their high performance and ability to handle complex data patterns.

Here are some common datasets suitable for practicing Boosting and SVMs:

## 1. MNIST Handwritten Digits Dataset

*   **Description:** A large database of handwritten digits (0-9). It consists of 60,000 training images and 10,000 testing images. Each image is a 28x28 pixel grayscale image.
*   **Features:** 784 numerical features (pixel values).
*   **Target:** Multi-class classification (10 digits).
*   **Use Case:** A classic benchmark for image classification. SVMs (especially with RBF kernels) and Boosting algorithms (like XGBoost) can achieve very high accuracy on MNIST, demonstrating their power on structured data.

## 2. Credit Default Prediction Dataset

*   **Description:** This dataset contains information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients in Taiwan. The goal is to predict credit default.
*   **Features:** A mix of numerical and categorical features (e.g., limit balance, sex, education, marriage, age, payment history, bill amounts, previous payments).
*   **Target:** Binary classification: default (1) or no default (0).
*   **Use Case:** A challenging real-world dataset for binary classification. Boosting algorithms (XGBoost, LightGBM) are often top performers on such tabular datasets, and SVMs can also provide strong results.

## 3. Covertype Dataset

*   **Description:** Predicts forest cover type from cartographic variables. It contains 581,012 observations and 54 attributes.
*   **Features:** A mix of numerical and binary categorical features (e.g., elevation, aspect, slope, horizontal distance to hydrology, vertical distance to hydrology, horizontal distance to roadways, hillshade, wilderness area, soil type).
*   **Target:** Multi-class classification (7 forest cover types).
*   **Use Case:** A large-scale dataset that benefits from efficient Boosting implementations. SVMs can also be applied, but might be more computationally intensive on such a large dataset without careful optimization.

## 4. Adult Income Dataset (Census Income)

*   **Description:** Extracts a subset of the US Census Bureau database. The task is to predict whether a person makes over 50K a year.
*   **Features:** A mix of numerical and categorical features (e.g., age, workclass, fnlwgt, education, education-num, marital-status, occupation, relationship, race, sex, capital-gain, capital-loss, hours-per-week, native-country).
*   **Target:** Binary classification: income >50K or <=50K.
*   **Use Case:** Good for practicing with a mix of feature types and for comparing the performance of various classification algorithms, including Boosting and SVMs.

## 5. Make Moons / Make Circles (Synthetic Datasets)

*   **Description:** These are synthetic 2D datasets generated by `scikit-learn` that are not linearly separable.
    *   `make_moons`: Generates two interleaving half-circles.
    *   `make_circles`: Generates a larger circle containing a smaller circle.
*   **Features:** 2 numerical features.
*   **Target:** Binary classification.
*   **Use Case:** Perfect for visually demonstrating the power of SVMs with non-linear kernels (like RBF) and how they can find complex decision boundaries that linear models cannot. Boosting algorithms can also learn these complex boundaries.

These datasets provide a range of complexities and sizes, allowing for thorough experimentation with Boosting and SVM algorithms.
