# Month 2, Week 3: Decision Trees & Bagging

## 1. Decision Trees

**Decision Trees** are non-parametric supervised learning algorithms used for both classification and regression tasks. They are intuitive, easy to understand, and mimic human decision-making processes.

### a) How Decision Trees Work
A decision tree makes predictions by learning simple decision rules inferred from the data features. It works by recursively splitting the data based on feature values until a stopping criterion is met (e.g., nodes are pure, maximum depth is reached).

*   **Nodes:**
    *   **Root Node:** The starting point of the tree, representing the entire dataset.
    *   **Internal Nodes:** Represent a test on a feature (e.g., "Is X > 1.6?"). Each branch from an internal node corresponds to an outcome of the test.
    *   **Leaf Nodes:** Terminal nodes that represent the final decision or prediction (e.g., a class label or a continuous value).
*   **Traversal:** To classify a new data point, you start at the root and follow the path down the tree based on the feature tests until you reach a leaf node, which gives the prediction.

### b) Building a Decision Tree: Splitting Criteria
The core challenge in building a decision tree is deciding how to split the data at each internal node. The goal is to create splits that maximize the "purity" of the child nodes (i.e., make them as homogeneous as possible with respect to the target variable).

*   **Impurity Measures:** Quantify the heterogeneity of a node.
    *   **Gini Impurity:** Measures how often a randomly chosen element from the set would be incorrectly labeled if it were randomly labeled according to the distribution of labels in the subset.
        `Gini = 1 - Σ (pᵢ)²` (where `pᵢ` is the proportion of class `i`)
    *   **Entropy:** Measures the uncertainty or randomness in a node. A node with high entropy is very mixed.
        `Entropy = - Σ pᵢ * log₂(pᵢ)`
*   **Information Gain:** The reduction in impurity (or increase in information) achieved by a split. The algorithm selects the split (feature and threshold) that maximizes information gain.
    `Information Gain = Impurity(Parent) - Σ (Weighted_Impurity(Child))`
*   **Greedy Approach:** Decision trees are typically built using a greedy approach, where the best split is chosen at each step without considering future splits. This does not guarantee a globally optimal tree but is computationally efficient.

### c) Overfitting in Decision Trees
Individual decision trees are prone to **overfitting**, especially if they are allowed to grow very deep. A deep tree can learn the training data too well, including noise, leading to poor generalization on unseen data. Techniques like pruning (removing branches) or setting a maximum depth can help mitigate this.

## 2. Ensemble Methods: Bagging

**Ensemble methods** combine multiple individual models (often called "weak learners") to produce a single, more robust, and accurate model. The idea is that the collective wisdom of many models is better than any single model.

**Bagging (Bootstrap Aggregating)** is a general-purpose ensemble method that aims to reduce the variance of a model, thereby helping to prevent overfitting.

### a) Bootstrap Sampling
*   From the original training dataset (of size `N`), `M` new training subsets (called "bootstrap samples") are created.
*   Each bootstrap sample is generated by randomly sampling `N` data points from the original dataset **with replacement**. This means some original data points may appear multiple times in a bootstrap sample, while others may not appear at all.
*   Typically, each bootstrap sample contains about 63.2% of the unique original data points.

### b) Model Training
*   A separate base model (e.g., a decision tree) is trained independently on each of the `M` bootstrap samples. These base models are often trained to be deep and complex, making them high-variance, low-bias learners.

### c) Aggregating Predictions
*   For a new, unseen data point, each of the `M` base models makes a prediction.
*   **For Regression:** The final prediction is the average of the individual model predictions.
*   **For Classification:** The final prediction is determined by majority voting among the individual model predictions.

## 3. Random Forests

**Random Forests** are an extension of bagging specifically designed for decision trees. They combine the concept of bagging with an additional layer of randomness to further decorrelate the individual trees, leading to even better performance and robustness.

### a) How Random Forests Work
1.  **Bootstrap Samples:** Similar to bagging, each tree in the forest is trained on a different bootstrap sample of the data.
2.  **Random Subset of Features:** Crucially, at each split point in the construction of an individual tree, only a random subset of the available features is considered as candidates for the split. This means that even if there's a very strong predictor feature, not all trees will necessarily split on it first, leading to more diverse trees.
3.  **Tree Growth:** Each tree is typically grown to its maximum possible depth (or until a minimum number of samples per leaf is reached) without pruning. This makes individual trees high-variance, low-bias learners.
4.  **Aggregation:** Predictions from all individual trees are aggregated (majority vote for classification, average for regression) to form the final prediction.

### b) Advantages of Random Forests
*   **Reduced Overfitting:** The combination of bagging and random feature selection significantly reduces the variance of the model, making it less prone to overfitting than individual decision trees.
*   **High Accuracy:** Often achieve very high accuracy and are robust to noise.
*   **Handles High-Dimensional Data:** Can work well with datasets containing a large number of features.
*   **Feature Importance:** Can provide estimates of feature importance, indicating which features are most influential in making predictions.
*   **Handles Missing Values:** Can be adapted to handle missing data effectively (e.g., using proximity measures).

### c) Out-of-Bag (OOB) Error
*   Since each tree in a random forest is trained on a bootstrap sample, approximately one-third of the original training data points are *not* included in that tree's training set. These are called "out-of-bag" (OOB) samples.
*   The OOB samples can be used to estimate the generalization error of the random forest without the need for a separate validation set. For each OOB sample, a prediction is made by only the trees that did *not* see that sample during training. These predictions are then aggregated, and the OOB error is calculated.

---
## Recommended Textbooks

*   **ISLR** – Chapter 8 (Trees & Ensembles)
*   **Hands-On Machine Learning** – Random Forests & Ensembles
*   **Ensemble Methods in Machine Learning** (Zhi-Hua Zhou)
