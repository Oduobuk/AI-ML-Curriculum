{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression and KNN in Practice\n",
    "\n",
    "This notebook demonstrates practical implementation of Logistic Regression and K-Nearest Neighbors (KNN) algorithms using scikit-learn. We'll work through a complete machine learning workflow from data loading to model evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, classification_report,\n",
    "    roc_curve, auc, precision_recall_curve, average_precision_score\n",
    ")\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Explore the Dataset\n",
    "We'll use the Breast Cancer Wisconsin dataset, a classic binary classification dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target, name='target')\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(\"\\nFirst 5 rows of the dataset:\")\n",
    "display(X.head())\n",
    "\n",
    "# Check class distribution\n",
    "print(\"\\nClass distribution:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# Visualize feature distributions\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=X.iloc[:, :5])\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Feature Distributions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Handle class imbalance using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Original training set shape: {X_train.shape}\")\n",
    "print(f\"Resampled training set shape: {X_train_resampled.shape}\")\n",
    "print(\"\\nClass distribution after resampling:\")\n",
    "print(pd.Series(y_train_resampled).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the model\n",
    "log_reg = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "log_reg.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_log = log_reg.predict(X_test_scaled)\n",
    "y_pred_proba_log = log_reg.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Logistic Regression Results:\")\n",
    "print(\"=\\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_log):.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_log))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_log))\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba_log)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the optimal k using cross-validation\n",
    "k_values = list(range(1, 31, 2))\n",
    "cv_scores = []\n",
    "\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "# Plot accuracy vs k\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, cv_scores, marker='o')\n",
    "plt.xlabel('Number of Neighbors (k)')\n",
    "plt.ylabel('Cross-Validated Accuracy')\n",
    "plt.title('KNN: Accuracy vs. k')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Train the model with optimal k\n",
    "optimal_k = k_values[np.argmax(cv_scores)]\n",
    "print(f\"Optimal number of neighbors: {optimal_k}\")\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=optimal_k)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_knn = knn.predict(X_test_scaled)\n",
    "y_pred_proba_knn = knn.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\nKNN Results:\")\n",
    "print(\"=\\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_knn):.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_knn))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare ROC curves\n",
    "fpr_log, tpr_log, _ = roc_curve(y_test, y_pred_proba_log)\n",
    "fpr_knn, tpr_knn, _ = roc_curve(y_test, y_pred_proba_knn)\n",
    "roc_auc_log = auc(fpr_log, tpr_log)\n",
    "roc_auc_knn = auc(fpr_knn, tpr_knn)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr_log, tpr_log, color='blue', lw=2, label=f'Logistic Regression (AUC = {roc_auc_log:.2f})')\n",
    "plt.plot(fpr_knn, tpr_knn, color='green', lw=2, label=f'KNN (AUC = {roc_auc_knn:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve Comparison')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': np.abs(log_reg.coef_[0])\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "# Plot top 10 most important features\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance.head(10))\n",
    "plt.title('Top 10 Most Important Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Hyperparameter Tuning (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Tuning Logistic Regression\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear', 'saga']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    LogisticRegression(max_iter=1000, random_state=42),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation accuracy: {:.4f}\".format(grid_search.best_score_))\n",
    "print(\"Test set accuracy: {:.4f}\".format(grid_search.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "In this notebook, we've covered:\n",
    "1. Data loading and exploration\n",
    "2. Data preprocessing and handling class imbalance\n",
    "3. Implementing and evaluating Logistic Regression\n",
    "4. Implementing and evaluating KNN\n",
    "5. Comparing model performance\n",
    "6. Feature importance analysis\n",
    "7. Hyperparameter tuning\n",
    "\n",
    "### Key Takeaways:\n",
    "- Logistic Regression works well with linearly separable data and provides interpretable coefficients.\n",
    "- KNN is a simple but powerful algorithm that works well with smaller datasets but can be computationally expensive.\n",
    "- Both models benefit from feature scaling.\n",
    "- Model evaluation should go beyond accuracy, especially with imbalanced datasets.\n",
    "\n",
    "### Next Steps:\n",
    "1. Try different classification algorithms (e.g., Random Forest, SVM)\n",
    "2. Experiment with feature selection techniques\n",
    "3. Explore more advanced evaluation metrics\n",
    "4. Deploy the model as a web application"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
