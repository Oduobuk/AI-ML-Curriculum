# Week 3: Advanced Data Analysis and Visualization

## Table of Contents
1. [Advanced Pandas Operations](#1-advanced-pandas-operations)
2. [Data Visualization Mastery](#2-data-visualization-mastery)
3. [Time Series Analysis](#3-time-series-analysis)
4. [Data Aggregation and Grouping](#4-data-aggregation-and-grouping)
5. [Project Implementation](#5-project-implementation)

## Introduction
Welcome to Week 3 of our Data Science curriculum! This week builds upon the foundational skills you've developed in Python, NumPy, and Pandas, taking your data analysis capabilities to the next level. We'll explore advanced techniques that will enable you to handle complex data manipulation tasks efficiently and create compelling visualizations that tell a story with your data.

### Learning Objectives
By the end of this week, you will be able to:
- Implement advanced data manipulation techniques using Pandas
- Create sophisticated visualizations with Matplotlib and Seaborn
- Perform time series analysis and handle temporal data effectively
- Optimize your code for better performance with large datasets
- Apply these skills to real-world data analysis projects

### Prerequisites
Before proceeding, ensure you're comfortable with:
- Basic Python programming (variables, loops, functions, lists, dictionaries)
- Working with NumPy arrays and basic operations
- Fundamental Pandas operations (DataFrames, Series, basic indexing)
- Creating simple plots with Matplotlib

### How to Use These Notes
- Follow along with the code examples in a Jupyter Notebook
- Try the exercises at the end of each section
- Refer to the official documentation for more details
- Experiment with different parameters and options

## 1. Advanced Pandas Operations

## 1. Advanced Pandas Operations

### 1.1 Multi-Indexing

MultiIndex (or hierarchical indexing) allows you to work with higher-dimensional data in a lower-dimensional structure. It's particularly useful when dealing with data that has multiple levels of hierarchy or when you need to represent data with more than two dimensions.

#### Creating MultiIndex Objects
```python
import pandas as pd
import numpy as np

# From tuples
arrays = [
    ['A', 'A', 'B', 'B', 'C', 'C'],
    [1, 2, 1, 2, 1, 2]
]
index = pd.MultiIndex.from_arrays(arrays, names=('letter', 'number'))

# From a DataFrame
df = pd.DataFrame({
    'A': np.random.randn(6),
    'B': np.random.randn(6)
}, index=index)

# From product
index = pd.MultiIndex.from_product(
    [['A', 'B', 'C'], [1, 2]],
    names=['letter', 'number']
)
```

#### Indexing and Selection
```python
# Selecting a single value
value = df.loc[('A', 1), 'A']

# Cross-section
cross = df.xs('A', level='letter')

# Slicing
sliced = df.loc[('A', 1):('B', 2)]
```

#### Stacking and Unstacking
```python
# Stacking (columns to index)
stacked = df.stack()

# Unstacking (index to columns)
unstacked = df.unstack(level=0)
```

#### Performance Considerations
- MultiIndex can increase memory usage
- Some operations may be slower with MultiIndex
- Consider resetting the index for certain operations
- Use `sort_index()` after modifications for better performance

### 1.2 Advanced Data Merging

Data merging is a fundamental operation in data analysis, allowing you to combine data from multiple sources. Pandas provides several methods for merging and concatenating data, each with its own use cases and performance characteristics.

#### Concatenation with Different Join Types

```python
import pandas as pd
import numpy as np

# Sample DataFrames
df1 = pd.DataFrame({
    'A': ['A0', 'A1', 'A2', 'A3'],
    'B': ['B0', 'B1', 'B2', 'B3'],
    'key': ['K0', 'K1', 'K2', 'K3']
})

df2 = pd.DataFrame({
    'C': ['C0', 'C1', 'C2', 'C3'],
    'D': ['D0', 'D1', 'D2', 'D3'],
    'key': ['K0', 'K1', 'K2', 'K3']
})

# Basic concatenation (vertical stacking)
result = pd.concat([df1, df2], axis=0)

# Horizontal concatenation
result = pd.concat([df1, df2], axis=1)

# Concatenation with different join types
result_outer = pd.concat([df1, df2], join='outer')
result_inner = pd.concat([df1, df2], join='inner')
```

#### Merging on Multiple Keys

```python
# Merge on a single key
merged = pd.merge(df1, df2, on='key')

# Merge on multiple keys
merged_multi = pd.merge(
    df1, 
    df2, 
    left_on=['key1', 'key2'],
    right_on=['key1', 'key2']
)

# Different join types
left_join = pd.merge(df1, df2, on='key', how='left')
right_join = pd.merge(df1, df2, on='key', how='right')
outer_join = pd.merge(df1, df2, on='key', how='outer')
```

#### Handling Overlapping Columns

```python
# Suffixes for overlapping columns
merged = pd.merge(
    df1, 
    df2, 
    on='key',
    suffixes=('_left', '_right')
)

# Using join method
joined = df1.set_index('key').join(df2.set_index('key'), lsuffix='_left', rsuffix='_right')

# Combining columns after merge
merged['combined'] = merged['col1'].fillna('') + merged['col2'].fillna('')
```

#### Performance Optimization

```python
# Use merge with sort=False for better performance
fast_merge = pd.merge(df1, df2, sort=False)

# Set index before merging
fast_merge = df1.set_index('key').join(df2.set_index('key'))

# Using merge_ordered for ordered data
ordered_merge = pd.merge_ordered(df1, df2, on='key')

# Using merge_asof for approximate matches
approx_merge = pd.merge_asof(df1, df2, on='datetime_col')
```

#### Best Practices
1. Always specify the `on` parameter explicitly
2. Be mindful of duplicate keys
3. Use appropriate join types (inner, outer, left, right)
4. Consider using `merge_ordered` or `merge_asof` for time series data
5. For large datasets, consider using Dask or PySpark

### 1.3 Pivot Tables and Cross-Tabulations

Pivot tables are a powerful way to summarize and analyze data in a tabular format. They allow you to transform and aggregate data to reveal patterns and insights that might not be immediately apparent in the raw data.

#### Creating Basic Pivot Tables

```python
import pandas as pd
import numpy as np

# Sample data
data = {
    'Date': pd.date_range('2023-01-01', periods=100, freq='D'),
    'Category': np.random.choice(['A', 'B', 'C'], 100),
    'Region': np.random.choice(['North', 'South', 'East', 'West'], 100),
    'Sales': np.random.randint(100, 1000, 100),
    'Profit': np.random.uniform(10, 500, 100)
}
df = pd.DataFrame(data)

# Basic pivot table
pivot = pd.pivot_table(
    df,
    values='Sales',
    index='Date',
    columns='Region',
    aggfunc='sum'
)
```

#### Advanced Aggregation Functions

```python
# Multiple aggregation functions
pivot_multi = pd.pivot_table(
    df,
    values=['Sales', 'Profit'],
    index='Region',
    columns='Category',
    aggfunc={
        'Sales': ['sum', 'mean', 'count'],
        'Profit': [np.mean, np.median, 'std']
    }
)

# Custom aggregation
def profit_margin(series):
    return (series['Profit'].sum() / series['Sales'].sum()) * 100

pivot_custom = df.groupby('Region').apply(profit_margin)
```

#### Handling Missing Values

```python
# Fill missing values
pivot_filled = pd.pivot_table(
    df,
    values='Sales',
    index='Date',
    columns='Region',
    fill_value=0,
    dropna=False
)

# Forward fill missing values
pivot_ffill = pivot.ffill(axis=0)
```

#### Performance Considerations

```python
# Using categorical data for better performance
df['Category'] = df['Category'].astype('category')
df['Region'] = df['Region'].astype('category')

# Using crosstab for simple frequency tables
freq_table = pd.crosstab(
    index=df['Category'],
    columns=df['Region'],
    margins=True,
    margins_name='Total'
)

# Using pivot_table with margins
pivot_with_margins = pd.pivot_table(
    df,
    values='Sales',
    index='Category',
    columns='Region',
    aggfunc='sum',
    margins=True,
    margins_name='Grand Total'
)
```

#### Real-world Example: Sales Analysis

```python
# Monthly sales by region and category
monthly_sales = pd.pivot_table(
    df,
    values='Sales',
    index=[df['Date'].dt.month_name(), 'Category'],
    columns='Region',
    aggfunc='sum',
    fill_value=0
).style.background_gradient(cmap='YlGnBu')

# Calculate percentage of total sales
total_sales = df['Sales'].sum()
sales_pct = (monthly_sales / total_sales * 100).round(2)
```

#### Best Practices
1. Use appropriate aggregation functions for your data
2. Be mindful of memory usage with large pivot tables
3. Consider using `margins` for quick totals
4. Use `fill_value` to handle missing data appropriately
5. For very large datasets, consider using `pivot_table` with `dask.dataframe`

## 2. Data Visualization Mastery

Effective data visualization is crucial for exploring and communicating insights from your data. This section covers advanced techniques for creating publication-quality visualizations using Matplotlib and Seaborn.

### 2.1 Advanced Matplotlib Customization

Matplotlib provides fine-grained control over every aspect of your visualizations. Understanding its object-oriented API is key to creating sophisticated plots.

#### Figure and Axes Customization

```python
import matplotlib.pyplot as plt
import numpy as np

# Create a figure with custom size and DPI
fig = plt.figure(figsize=(10, 6), dpi=100, facecolor='white')

# Add subplots with custom spacing
gs = fig.add_gridspec(
    nrows=2, 
    ncols=2,
    width_ratios=[2, 1],
    height_ratios=[1, 2],
    hspace=0.4,
    wspace=0.3
)

# Create subplots
ax1 = fig.add_subplot(gs[0, 0])
ax2 = fig.add_subplot(gs[0, 1])
ax3 = fig.add_subplot(gs[1, :])

# Customize figure background
fig.patch.set_facecolor('#f5f5f5')

# Add a main title
fig.suptitle('Advanced Figure Layout', fontsize=16, y=1.02)
```

#### Customizing Ticks and Labels

```python
# Sample data
x = np.linspace(0, 10, 100)
y = np.sin(x)

# Create plot
fig, ax = plt.subplots(figsize=(10, 5))
ax.plot(x, y, 'b-', linewidth=2)

# Customize ticks
from matplotlib.ticker import MultipleLocator, FormatStrFormatter

# Set major and minor ticks
ax.xaxis.set_major_locator(MultipleLocator(2))
ax.xaxis.set_major_formatter(FormatStrFormatter('%.1f'))
ax.xaxis.set_minor_locator(MultipleLocator(0.5))

# Customize tick labels
ax.tick_params(
    axis='both',
    which='both',
    direction='in',
    top=True,
    right=True,
    labelsize=10,
    grid_alpha=0.3
)

# Add grid
ax.grid(True, which='both', linestyle='--', alpha=0.3)

# Customize labels
ax.set_xlabel('Time (s)', fontsize=12, labelpad=10)
ax.set_ylabel('Amplitude', fontsize=12, labelpad=10)
ax.set_title('Customized Plot', pad=20)
```

#### Working with Legends and Annotations

```python
# Create sample data
x = np.linspace(0, 10, 100)
y1 = np.sin(x)
y2 = np.cos(x)

# Create plot
fig, ax = plt.subplots(figsize=(10, 6))
line1, = ax.plot(x, y1, 'b-', label='sin(x)')
line2, = ax.plot(x, y2, 'r--', label='cos(x)')

# Add legend with custom properties
legend = ax.legend(
    loc='upper right',
    frameon=True,
    fancybox=True,
    shadow=True,
    framealpha=0.9,
    fontsize=10,
    title='Functions',
    title_fontsize=12
)

# Add annotations
ax.annotate(
    'Local Max',
    xy=(np.pi/2, 1),
    xytext=(np.pi/2 + 1, 0.8),
    arrowprops=dict(facecolor='black', shrink=0.05, width=1, headwidth=8),
    fontsize=10
)

# Add text with bounding box
ax.text(
    0.02, 0.95,
    'Important Note:',
    transform=ax.transAxes,
    fontsize=10,
    verticalalignment='top',
    bbox=dict(
        boxstyle='round',
        facecolor='white',
        alpha=0.8,
        edgecolor='gray',
        pad=0.5
    )
)
```

#### Saving High-Quality Figures

```python
# Save figure with high resolution
fig.savefig(
    'high_quality_plot.png',
    dpi=300,
    bbox_inches='tight',
    pad_inches=0.1,
    transparent=False,
    facecolor='white',
    edgecolor='none'
)

# Save in different formats
formats = ['png', 'pdf', 'svg', 'eps']
for fmt in formats:
    fig.savefig(f'plot.{fmt}', format=fmt, dpi=300)

# Save with different backends (for publication quality)
import matplotlib
matplotlib.use('pgf')  # or 'cairo', 'svg', etc.
plt.rcParams.update({
    'pgf.texsystem': 'pdflatex',
    'font.family': 'serif',
    'text.usetex': True,
    'pgf.rcfonts': False,
})
```

### 2.2 Advanced Seaborn Visualizations

Seaborn builds on Matplotlib and provides a high-level interface for creating attractive statistical graphics. It works seamlessly with Pandas DataFrames and provides several built-in themes and color palettes.

#### Categorical Plots

```python
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# Load sample dataset
tips = sns.load_dataset('tips')

# Create a categorical plot with multiple layers
plt.figure(figsize=(10, 6))
ax = sns.boxplot(
    x='day',
    y='total_bill',
    hue='sex',
    data=tips,
    palette='Set2',
    width=0.6,
    linewidth=1,
    fliersize=3,
    notch=True
)

# Add swarm plot on top
sns.swarmplot(
    x='day',
    y='total_bill',
    hue='sex',
    data=tips,
    dodge=True,
    size=4,
    palette='dark:black',
    edgecolor='gray',
    linewidth=0.5
)

# Customize the plot
plt.title('Daily Total Bill Distribution by Gender', pad=20)
plt.xlabel('Day of Week', labelpad=10)
plt.ylabel('Total Bill ($)', labelpad=10)
plt.legend(title='Gender')
sns.despine(offset=10, trim=True)
plt.tight_layout()
```

#### Distribution Plots

```python
# Create a distribution plot with multiple elements
plt.figure(figsize=(10, 6))

# KDE plot
sns.kdeplot(
    data=tips,
    x='total_bill',
    hue='time',
    fill=True,
    alpha=0.2,
    linewidth=1.5,
    palette='viridis'
)

# Add rug plot
sns.rugplot(
    data=tips,
    x='total_bill',
    hue='time',
    height=0.05,
    palette='viridis',
    legend=False
)

# Add mean lines
for time, color in zip(['Lunch', 'Dinner'], sns.color_palette('viridis', 2)):
    mean_val = tips[tips['time'] == time]['total_bill'].mean()
    plt.axvline(mean_val, color=color, linestyle='--', linewidth=1.5)
    plt.text(
        mean_val + 0.5, 0.01,
        f'Mean ({time}): ${mean_val:.2f}',
        color=color,
        fontsize=10
    )

plt.title('Distribution of Total Bill by Meal Time', pad=15)
plt.xlabel('Total Bill ($)', labelpad=10)
plt.ylabel('Density', labelpad=10)
plt.legend(title='Meal Time')
sns.despine()
plt.tight_layout()
```

#### Regression Plots

```python
# Create a regression plot with confidence intervals
plt.figure(figsize=(10, 6))

# Scatter plot with regression line
sns.regplot(
    x='total_bill',
    y='tip',
    data=tips,
    scatter_kws={
        's': 60,
        'alpha': 0.7,
        'edgecolor': 'w',
        'linewidth': 0.5
    },
    line_kws={
        'color': 'red',
        'linewidth': 2
    },
    ci=95,  # 95% confidence interval
    order=2,  # Polynomial order
    truncate=False
)

# Add a residual plot
plt.figure(figsize=(10, 4))
sns.residplot(
    x='total_bill',
    y='tip',
    data=tips,
    lowess=True,
    line_kws={'color': 'red', 'linewidth': 2},
    scatter_kws={'alpha': 0.6}
)
plt.axhline(y=0, color='gray', linestyle='--', linewidth=1)
plt.title('Residual Plot')
plt.xlabel('Total Bill ($)')
plt.ylabel('Residuals')
sns.despine()
plt.tight_layout()
```

#### Matrix and Grid Visualizations

```python
# Create a pair plot with customizations
g = sns.pairplot(
    data=tips,
    hue='time',
    palette='viridis',
    corner=True,
    diag_kind='kde',
    plot_kws={
        's': 40,
        'alpha': 0.8,
        'edgecolor': 'w',
        'linewidth': 0.5
    },
    diag_kws={
        'fill': True,
        'alpha': 0.3
    }
)

# Adjust the layout
g.fig.suptitle('Pairwise Relationships in Tips Dataset', y=1.02)
plt.tight_layout()

# Create a heatmap with correlation matrix
plt.figure(figsize=(10, 8))
corr = tips.corr()
mask = np.triu(np.ones_like(corr, dtype=bool))
sns.heatmap(
    corr,
    mask=mask,
    annot=True,
    cmap='coolwarm',
    center=0,
    square=True,
    linewidths=0.5,
    cbar_kws={
        'shrink': 0.8,
        'label': 'Correlation'
    },
    annot_kws={
        'size': 10,
        'weight': 'bold'
    }
)
plt.title('Correlation Heatmap', pad=20)
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
```

#### Best Practices for Seaborn

1. **Choose the Right Plot Type**
   - Use boxplots/violin plots for distributions
   - Use scatter plots for relationships
   - Use bar plots for comparisons
   - Use heatmaps for matrices

2. **Color Usage**
   - Use colorblind-friendly palettes
   - Be consistent with color mappings
   - Use color to highlight important information

3. **Faceting**
   - Use `FacetGrid` for multi-plot grids
   - Keep consistent scales when comparing facets
   - Use meaningful titles and labels

4. **Performance**
   - For large datasets, consider using `jointplot` with `kind='hex'`
   - Use `sns.despine()` for cleaner plots
   - Cache data when working with large datasets

5. **Customization**
   - Use context managers for consistent styling
   - Customize titles and labels for clarity
   - Add annotations to highlight key points

```python
# Example of using context manager for consistent styling
with sns.axes_style('whitegrid'), sns.plotting_context('notebook', font_scale=1.1):
    g = sns.relplot(
        data=tips,
        x='total_bill',
        y='tip',
        col='time',
        hue='sex',
        style='smoker',
        size='size',
        sizes=(40, 200),
        alpha=0.8,
        palette='viridis',
        height=5,
        aspect=1.2
    )
    
    # Customize the plot
    g.set_axis_labels('Total Bill ($)', 'Tip ($)')
    g.fig.suptitle('Tips by Total Bill, Split by Time and Gender', y=1.05)
    g.add_legend(title='Gender/Smoker')
    
    # Adjust layout
    plt.tight_layout()
```

### 2.3 Interactive Visualizations with Plotly

Plotly is a powerful library for creating interactive, publication-quality visualizations. It offers a high-level interface (Plotly Express) for quick plotting and a lower-level interface (graph_objects) for complete control.

#### Introduction to Plotly

```python
import plotly.express as px
import plotly.graph_objects as go
import plotly.io as pio
from plotly.subplots import make_subplots
import pandas as pd
import numpy as np

# Set default template
pio.templates.default = 'plotly_white'

# Load sample dataset
tips = px.data.tips()
```

#### Creating Basic Interactive Plots

```python
# Scatter plot with hover information
fig = px.scatter(
    tips,
    x='total_bill',
    y='tip',
    color='sex',
    size='size',
    hover_data=['day', 'time', 'smoker'],
    title='Tips vs Total Bill',
    labels={
        'total_bill': 'Total Bill ($)',
        'tip': 'Tip ($)',
        'sex': 'Gender'
    },
    color_discrete_map={
        'Male': '#636EFA',
        'Female': '#EF553B'
    },
    template='plotly_white',
    width=800,
    height=500
)

# Update layout
fig.update_layout(
    title_font_size=24,
    title_x=0.5,
    xaxis_title_font_size=14,
    yaxis_title_font_size=14,
    legend_title_text='Gender',
    hoverlabel=dict(
        bgcolor='white',
        font_size=12,
        font_family='Arial'
    )
)

# Add shapes and annotations
fig.add_shape(
    type='line',
    x0=0, y0=0,
    x1=50, y1=10,
    line=dict(color='gray', width=2, dash='dash')
)

fig.add_annotation(
    x=40, y=8,
    text='10% tip line',
    showarrow=True,
    arrowhead=2
)

# Show the figure
fig.show()
```

#### Advanced Interactive Features

```python
# Create a figure with multiple subplots
fig = make_subplots(
    rows=2, cols=2,
    subplot_titles=(
        'Total Bill Distribution',
        'Tip Distribution',
        'Total Bill by Day',
        'Tip by Meal Time'
    ),
    specs=[[{'type': 'box'}, {'type': 'box'}],
           [{'type': 'bar'}, {'type': 'violin'}]]
)

# Add traces
fig.add_trace(
    go.Box(y=tips['total_bill'], name='Total Bill'),
    row=1, col=1
)

fig.add_trace(
    go.Box(y=tips['tip'], name='Tip'),
    row=1, col=2
)

# Add bar chart
day_totals = tips.groupby('day')['total_bill'].sum().reset_index()
fig.add_trace(
    go.Bar(
        x=day_totals['day'],
        y=day_totals['total_bill'],
        marker_color='#636EFA',
        name='Total Bill by Day'
    ),
    row=2, col=1
)

# Add violin plot
fig.add_trace(
    go.Violin(
        x=tips['time'],
        y=tips['tip'],
        box_visible=True,
        meanline_visible=True,
        points='all',
        name='Tip by Meal Time',
        jitter=0.1,
        pointpos=-1.8
    ),
    row=2, col=2
)

# Update layout
fig.update_layout(
    title='Interactive Dashboard',
    showlegend=False,
    height=700,
    template='plotly_white'
)

# Add dropdown menu
fig.update_layout(
    updatemenus=[
        dict(
            buttons=list([
                dict(
                    args=[{'visible': [True, True, True, True]}],
                    label='Show All',
                    method='update'
                ),
                dict(
                    args=[{'visible': [True, False, False, False]}],
                    label='Distribution Only',
                    method='update'
                ),
                dict(
                    args=[{'visible': [False, False, True, True]}],
                    label='Categorical Only',
                    method='update'
                )
            ]),
            direction='down',
            showactive=True,
            x=1.0,
            xanchor='right',
            y=1.15,
            yanchor='top'
        ),
    ]
)

fig.show()
```

#### Creating Dashboards with Plotly Dash

```python
# Install required packages if needed
# !pip install dash dash-bootstrap-components

import dash
from dash import dcc, html
from dash.dependencies import Input, Output
import dash_bootstrap_components as dbc

# Initialize the app
app = dash.Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])

# Create the app layout
app.layout = dbc.Container([
    dbc.Row([
        dbc.Col(html.H1("Restaurant Tips Dashboard"), className="text-center mb-4")
    ]),
    
    dbc.Row([
        dbc.Col([
            html.Label("Select Day:"),
            dcc.Dropdown(
                id='day-dropdown',
                options=[{'label': day, 'value': day} for day in tips['day'].unique()],
                value=['Thur', 'Fri', 'Sat', 'Sun'],
                multi=True
            )
        ], width=6),
        
        dbc.Col([
            html.Label("Select Time:"),
            dcc.RadioItems(
                id='time-radio',
                options=[
                    {'label': 'Lunch', 'value': 'Lunch'},
                    {'label': 'Dinner', 'value': 'Dinner'},
                    {'label': 'Both', 'value': 'Both'}
                ],
                value='Both',
                labelStyle={'display': 'inline-block', 'margin-right': '10px'}
            )
        ], width=6)
    ], className="mb-4"),
    
    dbc.Row([
        dbc.Col(dcc.Graph(id='scatter-plot'), width=8),
        dbc.Col([
            dcc.Graph(id='pie-chart'),
            dcc.Graph(id='box-plot')
        ], width=4)
    ])
])

# Callbacks for interactivity
@app.callback(
    [Output('scatter-plot', 'figure'),
     Output('pie-chart', 'figure'),
     Output('box-plot', 'figure')],
    [Input('day-dropdown', 'value'),
     Input('time-radio', 'value')]
)
def update_plots(selected_days, selected_time):
    # Filter data
    filtered_df = tips[tips['day'].isin(selected_days)]
    if selected_time != 'Both':
        filtered_df = filtered_df[filtered_df['time'] == selected_time]
    
    # Create scatter plot
    scatter_fig = px.scatter(
        filtered_df,
        x='total_bill',
        y='tip',
        color='sex',
        size='size',
        hover_data=['day', 'time', 'smoker'],
        title='Tips vs Total Bill',
        labels={'total_bill': 'Total Bill ($)', 'tip': 'Tip ($)', 'sex': 'Gender'}
    )
    
    # Create pie chart
    pie_fig = px.pie(
        filtered_df,
        names='day',
        title='Proportion by Day',
        hole=0.4
    )
    
    # Create box plot
    box_fig = px.box(
        filtered_df,
        x='sex',
        y='total_bill',
        color='time',
        title='Total Bill by Gender and Time',
        labels={'sex': 'Gender', 'total_bill': 'Total Bill ($)', 'time': 'Meal Time'}
    )
    
    # Update layouts
    for fig in [scatter_fig, pie_fig, box_fig]:
        fig.update_layout(template='plotly_white')
    
    return scatter_fig, pie_fig, box_fig

# Run the app
if __name__ == '__main__':
    app.run_server(debug=True)
```

#### Exporting and Embedding Interactive Visualizations

```python
# Save as HTML
fig.write_html("interactive_plot.html", include_plotlyjs='cdn')

# Save as static image
fig.write_image("static_plot.png", scale=2)  # Requires kaleido

# Embed in Jupyter Notebook
from IPython.display import HTML
HTML(fig.to_html(include_plotlyjs='cdn'))

# Create a standalone HTML with all dependencies
pio.write_html(fig, 'standalone_plot.html', auto_open=True)
```

#### Best Practices for Interactive Visualizations

1. **Performance Optimization**
   - Use `px.scatter` with `render_mode='webgl'` for large datasets
   - Implement callbacks efficiently to minimize re-renders
   - Use `dcc.Store` to cache data in Dash apps

2. **User Experience**
   - Add clear labels and titles
   - Include tooltips with relevant information
   - Use consistent color schemes
   - Implement intuitive controls

3. **Responsive Design**
   - Use `responsive=True` for automatic resizing
   - Test on different screen sizes
   - Consider mobile interactions

4. **Accessibility**
   - Add alt text for screen readers
   - Ensure sufficient color contrast
   - Provide keyboard navigation

5. **Deployment**
   - Use `gunicorn` or `waitress` for production deployment
   - Set `debug=False` in production
   - Consider using a CDN for static assets

## 3. Time Series Analysis

Time series analysis is crucial for understanding temporal patterns in data. This section covers handling datetime objects, time-based indexing, resampling, and advanced time series operations using pandas.

### 3.1 Working with Datetime Objects

#### Parsing and Formatting Dates

```python
import pandas as pd
import numpy as np

# Create sample datetime strings
date_strings = [
    '2023-01-01', '2023-01-02', '2023-01-03',
    '01/04/2023', '01/05/2023', '2023-06-01 14:30:00'
]

# Convert to datetime
# Basic conversion
dates = pd.to_datetime(date_strings)
print("Basic conversion:")
print(dates)

# Handle different formats
dates_custom = pd.to_datetime(
    date_strings,
    format='mixed',  # Automatically infer format
    dayfirst=False,  # MM/DD vs DD/MM
    yearfirst=True  # YYYY-MM-DD format
)
print("\nCustom format conversion:")
print(dates_custom)

# Handle missing/invalid dates
dates_with_errors = [
    '2023-01-01', '2023-02-30',  # Invalid date
    '2023-03-15', 'not_a_date',   # Invalid format
    '2023-05-20'
]

dates_handled = pd.to_datetime(
    dates_with_errors,
    errors='coerce'  # Convert to NaT for errors
)
print("\nHandling errors:")
print(dates_handled)

# Format datetime objects
formatted_dates = dates_handled.dt.strftime('%Y-%m-%d %A')
print("\nFormatted dates:")
print(formatted_dates)
```

#### Timezone Handling

```python
# Timezone-naive vs timezone-aware
naive_dt = pd.Timestamp('2023-01-01 09:00')
print("\nTimezone-naive:", naive_dt.tz)

# Localize to a timezone
ny_dt = naive_dt.tz_localize('America/New_York')
print("NY time:", ny_dt)

# Convert between timezones
la_dt = ny_dt.tz_convert('America/Los_Angeles')
print("LA time:", la_dt)

# Handle daylight saving time
dst_dates = pd.date_range(
    start='2023-03-12',  # DST starts in US
    end='2023-03-13',
    freq='6H',
    tz='America/New_York'
)
print("\nDST transition:")
print(dst_dates)
```

#### Date Ranges and Frequencies

```python
# Create date ranges
# Basic daily range
daily_range = pd.date_range(
    start='2023-01-01',
    end='2023-01-07',
    freq='D'  # Daily frequency
)
print("\nDaily range:")
print(daily_range)

# Business days
bday_range = pd.date_range(
    start='2023-01-01',
    periods=10,
    freq='B'  # Business days
)
print("\nBusiness days:")
print(bday_range)

# Custom frequencies
custom_range = pd.date_range(
    start='2023-01-01',
    periods=5,
    freq='2W-MON'  # Every other Monday
)
print("\nCustom frequency (every other Monday):")
print(custom_range)

# Month ends
month_ends = pd.date_range(
    start='2023-01-01',
    end='2023-06-01',
    freq='M'  # Month end
)
print("\nMonth ends:")
print(month_ends)
```

#### Shifting and Lagging Time Series Data

```python
# Create sample time series
np.random.seed(42)
data = pd.Series(
    np.random.randn(10),
    index=pd.date_range('2023-01-01', periods=10, freq='D')
)
print("\nOriginal data:")
print(data)

# Simple shift
shifted = data.shift(1)  # Shift down by 1 period
print("\nShifted down by 1:")
print(shifted)

# Shift with frequency
shifted_freq = data.shift(1, freq='D')  # Shift index by 1 day
print("\nShifted index by 1 day:")
print(shifted_freq)

# Calculate percentage change
pct_change = data.pct_change()
print("\nPercentage change:")
print(pct_change)

# Rolling calculations
rolling_mean = data.rolling(window=3).mean()
print("\n3-day rolling mean:")
print(rolling_mean)

# Expanding window
expanding_sum = data.expanding().sum()
print("\nExpanding sum:")
print(expanding_sum)

# Time-based shifting
timedelta_shift = data.shift(periods=1, freq='D')
print("\nTime-based shift:")
print(timedelta_shift)
```

#### Best Practices for Working with Datetime Objects

1. **Consistent Timezones**
   - Always be aware of timezone information
   - Convert to UTC for storage and calculations
   - Localize only for display purposes

2. **Handling Missing Data**
   - Use appropriate fill methods for time series
   - Consider forward fill or interpolation for missing timestamps
   - Be cautious with `dropna()` in time series data

3. **Performance**
   - Use `infer_datetime_format=True` for faster parsing
   - Set columns as datetime during file reading when possible
   - Use `pd.to_datetime()` with `format` for known formats

4. **Date Ranges**
   - Use `pd.date_range()` for generating regular date sequences
   - Consider business day frequencies (`B`) for financial data
   - Be mindful of holidays and special calendar events

5. **Shifting and Differencing**
   - Understand the difference between `shift()` and `diff()`
   - Use `pct_change()` for percentage changes
   - Consider using `tshift()` for time-based shifting (deprecated in favor of `shift(freq=...)`)

### 3.2 Time-Based Indexing

Effective time series analysis in pandas relies heavily on proper indexing. This section covers how to work with DatetimeIndex, perform time-based selection, and handle resampling operations.

#### Setting and Using DatetimeIndex

```python
import pandas as pd
import numpy as np

# Create sample data
data = {
    'value': np.random.randn(1000),
    'category': np.random.choice(['A', 'B', 'C'], 1000)
}

# Basic datetime range
index = pd.date_range('2023-01-01', periods=1000, freq='H')
ts = pd.DataFrame(data, index=index)
print("Time series with hourly frequency:")
print(ts.head())

# Convert string index to DatetimeIndex
df = pd.DataFrame({
    'date': pd.date_range('2023-01-01', periods=5, freq='D'),
    'value': [10, 20, 30, 40, 50]
})

# Set datetime column as index
df_timeindex = df.set_index('date')
print("\nDataFrame with DatetimeIndex:")
print(df_timeindex)

# Partial string indexing
print("\nSelect January 2023 data:")
print(ts['2023-01'])
```

#### Slicing and Selecting Time Series Data

```python
# Basic slicing
print("First 5 rows:")
print(ts.iloc[:5])

# Time-based slicing
print("\nJanuary 1st data:")
print(ts['2023-01-01'])

# Between two dates
print("\nData between Jan 2 and Jan 3:")
print(ts['2023-01-02':'2023-01-03'])

# Using first() and last()
print("\nFirst 3 entries:")
print(ts.first('3H'))

print("\nLast 2 entries:")
print(ts.last('2H'))

# Using at_time and between_time
print("\nAll entries at 9:00 AM:")
print(ts.at_time('09:00'))

print("\nEntries between 9 AM and 12 PM:")
print(ts.between_time('09:00', '12:00'))

# Using truncate
print("\nData after Jan 5, 2023:")
print(ts.truncate(after='2023-01-05'))
```

#### Resampling and Frequency Conversion

```python
# Downsampling (hourly to daily)
daily_mean = ts.resample('D').mean()
print("\nDaily mean:")
print(daily_mean.head())

# Multiple aggregations
daily_stats = ts.resample('D').agg(['mean', 'min', 'max', 'std'])
print("\nDaily statistics:")
print(daily_stats.head())

# Upsampling (hourly to 15-minutely)
upsampled = ts.resample('15T').asfreq()
print("\nUpsampled to 15 minutes:")
print(upsampled.head())

# Forward fill for upsampling
ffilled = ts.resample('15T').ffill()
print("\nForward filled upsampling:")
print(ffilled.head())

# Time-based grouping
print("\nMean by hour of day:")
print(ts.groupby(ts.index.hour).mean())

# Resample with custom function
def custom_resampler(array):
    return array[-1]  # Return last value in period

custom_resampled = ts.resample('6H').apply(custom_resampler)
print("\nCustom resampling (last value in 6-hour window):")
print(custom_resampled.head())
```

#### Handling Missing Time Series Data

```python
# Create time series with missing values
rng = pd.date_range('2023-01-01', periods=10, freq='D')
data = pd.Series([1, np.nan, 3, np.nan, 5, 6, np.nan, 8, 9, 10], index=rng)
print("Original series with missing values:")
print(data)

# Forward fill
ffilled = data.ffill()
print("\nForward filled:")
print(ffilled)

# Backward fill
bfilled = data.bfill()
print("\nBackward filled:")
print(bfilled)

# Linear interpolation
interpolated = data.interpolate()
print("\nLinearly interpolated:")
print(interpolated)

# Time-based interpolation
ts_missing = ts.copy()
ts_missing.loc[ts_missing.sample(frac=0.1).index, 'value'] = np.nan

# Forward fill with limit
ts_ffill = ts_missing.ffill(limit=2)
print("\nForward fill with limit of 2:")
print(ts_ffill.isna().sum())

# Interpolate with time method
ts_time_interp = ts_missing.interpolate(method='time')
print("\nTime-based interpolation:")
print(ts_time_interp.isna().sum())

# Resampling with interpolation
ts_resampled = ts_missing.resample('15T').interpolate(method='linear')
print("\nResampled with interpolation:")
print(ts_resampled.head())

# Drop missing values
dropped = ts_missing.dropna()
print("\nDropped missing values:")
print(f"Original length: {len(ts_missing)}, After drop: {len(dropped)}")
```

#### Best Practices for Time-Based Indexing

1. **Index Consistency**
   - Always ensure your index is sorted
   - Check for duplicate indices with `df.index.duplicated().any()`
   - Use `asfreq()` to conform to a specific frequency

2. **Performance Optimization**
   - Use `loc` for label-based indexing
   - Prefer vectorized operations over loops
   - Consider using `at` and `iat` for scalar lookups

3. **Resampling**
   - Choose appropriate aggregation methods
   - Be mindful of edge cases (closed intervals, label positions)
   - Consider using `loffset` parameter to adjust timestamps

4. **Handling Missing Data**
   - Document your imputation strategy
   - Consider multiple imputation for critical analyses
   - Be cautious with forward/backward filling in volatile series

5. **Time Zones**
   - Store data in UTC when possible
   - Localize timezones only for display
   - Be aware of daylight saving time transitions

### 3.3 Time Series Decomposition

Time series decomposition is a technique that breaks down a time series into its constituent components: trend, seasonality, and residuals. This helps in understanding the underlying patterns and making more accurate forecasts.

#### Trend, Seasonality, and Residuals

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.seasonal import seasonal_decompose

# Generate sample time series with trend and seasonality
np.random.seed(42)
dates = pd.date_range('2020-01-01', periods=365*3, freq='D')
trend = np.linspace(0, 10, len(dates))
seasonality = 5 * np.sin(2 * np.pi * dates.dayofyear / 365)
noise = np.random.normal(0, 1, len(dates))
ts_values = trend + seasonality + noise

ts = pd.Series(ts_values, index=dates, name='value')

# Plot the components
plt.figure(figsize=(12, 8))

plt.subplot(4, 1, 1)
plt.plot(ts.index, ts.values, label='Original')
plt.title('Original Time Series')
plt.legend()

plt.subplot(4, 1, 2)
plt.plot(ts.index, trend, label='Trend')
plt.title('Trend Component')
plt.legend()

plt.subplot(4, 1, 3)
plt.plot(ts.index, seasonality, label='Seasonality')
plt.title('Seasonal Component')
plt.legend()

plt.subplot(4, 1, 4)
plt.plot(ts.index, noise, label='Residuals')
plt.title('Residual Component')
plt.legend()

plt.tight_layout()
plt.show()
```

#### Moving Averages and Smoothing

```python
# Simple Moving Average (SMA)
window_size = 30
ts_sma = ts.rolling(window=window_size, center=True).mean()

# Exponentially Weighted Moving Average (EWMA)
span = 30
ts_ewm = ts.ewm(span=span, adjust=False).mean()

# LOESS Smoothing (using statsmodels)
from statsmodels.nonparametric.smoothers_lowess import lowess
loess_smoothed = lowess(ts.values, ts.index.astype(np.int64) // 10**9, frac=0.1)
loess_series = pd.Series(loess_smoothed[:, 1], index=ts.index)

# Plot different smoothing techniques
plt.figure(figsize=(12, 6))
plt.plot(ts.index, ts, label='Original', alpha=0.3)
plt.plot(ts_sma.index, ts_sma, label=f'SMA ({window_size}-day)', linewidth=2)
plt.plot(ts_ewm.index, ts_ewm, label=f'EWMA (span={span})', linewidth=2)
plt.plot(loess_series.index, loess_series, label='LOESS (frac=0.1)', linewidth=2)
plt.title('Time Series Smoothing Techniques')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

#### Seasonal Decomposition of Time Series

```python
# Classical decomposition (multiplicative)
result_mul = seasonal_decompose(ts, model='multiplicative', period=365)

# Classical decomposition (additive)
result_add = seasonal_decompose(ts, model='additive', period=365)

# Plot the decomposition
plt.figure(figsize=(12, 10))

# Multiplicative model
plt.subplot(2, 2, 1)
result_mul.observed.plot(ax=plt.gca())
plt.title('Observed (Multiplicative)')

plt.subplot(2, 2, 2)
result_mul.trend.plot(ax=plt.gca())
plt.title('Trend (Multiplicative)')

plt.subplot(2, 2, 3)
result_mul.seasonal.plot(ax=plt.gca())
plt.title('Seasonal (Multiplicative)')

plt.subplot(2, 2, 4)
result_mul.resid.plot(ax=plt.gca())
plt.title('Residuals (Multiplicative)')

plt.tight_layout()
plt.show()

# Compare additive vs multiplicative
plt.figure(figsize=(12, 6))
result_add.seasonal.plot(label='Additive')
result_mul.seasonal.plot(label='Multiplicative')
plt.title('Additive vs Multiplicative Seasonal Components')
plt.legend()
plt.show()
```

#### Stationarity and Differencing

```python
from statsmodels.tsa.stattools import adfuller, kpss

def test_stationarity(timeseries, window=12):
    """Test for stationarity using rolling statistics and Dickey-Fuller test."""
    # Rolling statistics
    rolmean = timeseries.rolling(window=window).mean()
    rolstd = timeseries.rolling(window=window).std()
    
    # Plot rolling statistics
    plt.figure(figsize=(12, 6))
    orig = plt.plot(timeseries, color='blue', label='Original')
    mean = plt.plot(rolmean, color='red', label='Rolling Mean')
    std = plt.plot(rolstd, color='black', label='Rolling Std')
    plt.legend(loc='best')
    plt.title('Rolling Mean & Standard Deviation')
    plt.show()
    
    # Dickey-Fuller test
    print('Results of Dickey-Fuller Test:')
    dftest = adfuller(timeseries, autolag='AIC')
    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic', 'p-value', '#Lags Used', 'Number of Observations Used'])
    for key, value in dftest[4].items():
        dfoutput[f'Critical Value ({key})'] = value
    print(dfoutput)
    
    # KPSS test
    print('\nResults of KPSS Test:')
    kpsstest = kpss(timeseries, regression='c', nlags='auto')
    kpss_output = pd.Series(kpsstest[0:3], index=['Test Statistic', 'p-value', 'Lags Used'])
    for key, value in kpsstest[3].items():
        kpss_output[f'Critical Value ({key})'] = value
    print(kpss_output)

# Test original series
print("Testing original series:")
test_stationarity(ts)

# Apply differencing
ts_diff = ts.diff().dropna()
print("\nTesting first difference:")
test_stationarity(ts_diff)

# Seasonal differencing
ts_seasonal_diff = ts.diff(periods=365).dropna()
print("\nTesting seasonal difference (365 days):")
test_stationarity(ts_seasonal_diff)

# Log transform for multiplicative models
ts_log = np.log(ts)
ts_log_diff = ts_log.diff().dropna()
print("\nTesting log difference:")
test_stationarity(ts_log_diff)
```

#### Advanced Decomposition Techniques

```python
# STL Decomposition (Seasonal-Trend decomposition using LOESS)
from statsmodels.tsa.seasonal import STL

stl = STL(ts, period=365, robust=True)
result_stl = stl.fit()

# Plot STL decomposition
plt.figure(figsize=(12, 10))

plt.subplot(4, 1, 1)
plt.plot(result_stl.observed)
plt.title('Observed')

plt.subplot(4, 1, 2)
plt.plot(result_stl.trend)
plt.title('Trend')

plt.subplot(4, 1, 3)
plt.plot(result_stl.seasonal)
plt.title('Seasonal')

plt.subplot(4, 1, 4)
plt.plot(result_stl.resid)
plt.title('Residuals')

plt.tight_layout()
plt.show()

# Compare residuals from different methods
plt.figure(figsize=(12, 6))
plt.plot(result_add.resid, label='Additive Decomposition')
plt.plot(result_mul.resid, label='Multiplicative Decomposition')
plt.plot(result_stl.resid, label='STL Decomposition')
plt.title('Comparison of Residuals from Different Decomposition Methods')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

#### Best Practices for Time Series Decomposition

1. **Choosing the Right Model**
   - Use additive models when the seasonal variation is constant over time
   - Use multiplicative models when the seasonal variation changes with the level of the time series
   - Consider STL for complex seasonality or when robustness to outliers is needed

2. **Handling Missing Values**
   - Impute missing values before decomposition
   - Consider using STL with `robust=True` for data with outliers

3. **Seasonal Period**
   - Set the correct seasonal period (e.g., 12 for monthly data, 7 for daily data with weekly seasonality)
   - For multiple seasonal patterns, consider using MSTL (Multiple Seasonal-Trend decomposition)

4. **Residual Analysis**
   - Check if residuals are white noise (no autocorrelation)
   - Use Ljung-Box or Box-Pierce tests for residual autocorrelation
   - Consider additional modeling if significant patterns remain in residuals

5. **Transformation**
   - Apply log transformation for multiplicative patterns
   - Consider Box-Cox transformation for non-constant variance

6. **Forecasting**
   - Decompose the series before forecasting individual components
   - Recombine component forecasts appropriately (additive or multiplicative)
   - Consider using decomposition-based methods like TBATS for complex seasonality

## 4. Data Aggregation and Grouping

## 4. Data Aggregation and Grouping

### 4.1 Advanced GroupBy Operations
- Grouping with multiple keys
- Custom aggregation functions
- Transformation and filtering
- Performance optimization

### 4.2 Window Functions

## 5. Exercise Theory and Concepts

### Exercise 1: Advanced Pandas Operations

#### Theoretical Foundations

1. **Multi-indexing in Pandas**
   - **Concept**: Multi-indexing (hierarchical indexing) allows working with higher-dimensional data in a 2D structure
   - **Key Operations**:
     - `set_index()`: Convert columns to multi-index levels
     - `unstack()`: Pivot a level of hierarchical index labels
     - `stack()`: Pivot columns to index levels
     - `xs()`: Cross-section selection
   - **Use Cases**:
     - Working with panel data
     - Time series with multiple variables
     - Grouped aggregations with multiple keys

2. **Advanced Merging**
   - **Join Types**:
     - Inner join: Only matching keys from both DataFrames
     - Outer join: All keys from both DataFrames
     - Left/Right join: All keys from left/right DataFrame
   - **Key Parameters**:
     - `how`: Type of join
     - `on`: Column(s) to join on
     - `suffixes`: For overlapping column names
   - **Performance Considerations**:
     - Set indices before merging
     - Use `validate` parameter to check assumptions

3. **Pivot Tables**
   - **Purpose**: Reshape data and compute aggregations
   - **Key Parameters**:
     - `values`: Column to aggregate
     - `index`: Rows in output
     - `columns`: Columns in output
     - `aggfunc`: Aggregation function(s)
   - **Advanced Features**:
     - Multiple aggregation functions
     - Handling missing values
     - Margins for subtotals

### Exercise 2: Data Visualization

#### Theoretical Foundations

1. **Matplotlib Customization**
   - **Figure and Axes**: Understanding the object-oriented interface
   - **Styling Elements**:
     - Line styles, markers, colors
     - Text properties and annotations
     - Grid and axis formatting
   - **Subplots and Layouts**:
     - `plt.subplots()`
     - `GridSpec` for complex layouts
     - Constrained layout manager

2. **Seaborn Visualization**
   - **Statistical Plotting**:
     - Distribution plots
     - Categorical plots
     - Regression plots
   - **Figure-level vs Axes-level Functions**:
     - `displot` vs `histplot`
     - `catplot` vs `boxplot`/`violinplot`
   - **Styling**:
     - Color palettes
     - Plot themes
     - Context settings

3. **Interactive Visualization with Plotly**
   - **Core Concepts**:
     - Traces and layout
     - Hover interactions
     - Animation frames
   - **Express vs Graph Objects**:
     - When to use each
     - Converting between them
   - **Deployment**:
     - Standalone HTML
     - Dash applications
     - Jupyter integration

### Exercise 3: Time Series Analysis

#### Theoretical Foundations

1. **Time-based Indexing**
   - **DatetimeIndex**:
     - Creation and manipulation
     - Partial string indexing
     - Slicing with timestamps
   - **Date Ranges and Frequencies**:
     - Regular sequences
     - Business day calendars
     - Custom frequencies

2. **Resampling**
   - **Downsampling**:
     - Aggregation methods
     - Closed intervals
     - Label alignment
   - **Upsampling**:
     - Forward/backward fill
     - Interpolation methods
   - **Performance Considerations**:
     - Memory usage
     - Computational efficiency

3. **Rolling Windows**
   - **Window Types**:
     - Fixed windows
     - Expanding windows
     - Exponentially weighted windows
   - **Common Operations**:
     - Moving averages
     - Standard deviations
     - Custom functions

4. **Handling Missing Data**
   - **Detection Methods**:
     - `isna()`, `notna()`
     - Visual inspection
   - **Imputation Techniques**:
     - Forward/backward fill
     - Linear interpolation
     - Time-based interpolation
   - **Impact on Analysis**:
     - Statistical properties
     - Model assumptions

## 6. Code Lab Solutions Walkthrough

### Exercise 1: Advanced Pandas Operations

```python
# Exercise 1: Create a MultiIndex DataFrame
multi_df = df1.set_index(['date', 'category'])
print("MultiIndex DataFrame:")
print(multi_df)

# Exercise 2: Merge DataFrames
merged_df = pd.merge(df1, df2, on=['date', 'category'])
print("\nMerged DataFrame:")
print(merged_df)

# Exercise 3: Create pivot table
pivot_df = df1.pivot_table(
    values='value',
    index='date',
    columns='category',
    aggfunc='mean'
)
print("\nPivot Table:")
print(pivot_df)
```

### Exercise 2: Data Visualization

```python
# Exercise 1: Matplotlib scatter plot
plt.figure(figsize=(10, 6))
for day in tips['day'].unique():
    day_data = tips[tips['day'] == day]
    plt.scatter(day_data['total_bill'], day_data['tip'], label=day)
plt.title('Total Bill vs Tip by Day')
plt.xlabel('Total Bill ($)')
plt.ylabel('Tip ($)')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

# Exercise 2: Seaborn box plot
plt.figure(figsize=(12, 6))
sns.boxplot(x='day', y='total_bill', hue='time', data=tips)
plt.title('Distribution of Total Bill by Day and Time')
plt.tight_layout()
plt.show()

# Exercise 3: Interactive Plotly visualization
fig = px.scatter(
    tips,
    x='total_bill',
    y='tip',
    color='smoker',
    size='size',
    hover_data=['day', 'time'],
    title='Interactive Tips Visualization'
)
fig.show()
```

### Exercise 3: Time Series Analysis

```python
# Exercise 1: Resample to weekly frequency
weekly_mean = df['price'].resample('W').mean()
print("Weekly Mean Prices:")
print(weekly_mean.head())

# Exercise 2: 7-day rolling average
rolling_avg = df['price'].rolling(window=7).mean()
print("\n7-day Rolling Average:")
print(rolling_avg.head(10))

# Exercise 3: Monthly statistics
monthly_stats = df['price'].resample('M').agg(['min', 'max', 'mean'])
print("\nMonthly Statistics:")
print(monthly_stats)

# Exercise 4: Handle missing values
df_filled = df_missing.fillna(method='ffill')
print("\nDataFrame with Forward-filled Missing Values:")
print(df_filled.iloc[8:17])  # Show rows around filled values
```

## 7. Best Practices and Common Pitfalls

### Pandas Operations
- **Do**: Use vectorized operations instead of loops
- **Don't**: Use chained indexing (e.g., `df[df['a'] > 2]['b']`)
- **Watch out for**:
  - In-place modifications
  - SettingWithCopyWarning
  - Memory usage with large datasets

### Data Visualization
- **Do**:
  - Label your axes
  - Use appropriate scales
  - Include legends
- **Don't**:
  - Overcrowd plots
  - Use misleading scales
  - Forget to check color blindness accessibility

### Time Series Analysis
- **Do**:
  - Handle time zones explicitly
  - Check for missing dates
  - Consider seasonality
- **Don't**:
  - Ignore autocorrelation
  - Use future data for predictions
  - Overlook stationarity requirements
- Rolling windows
- Expanding windows
- Exponentially weighted windows
- Custom window functions

### 4.3 Performance Optimization
- Vectorized operations
  - Memory usage optimization
  - Parallel processing with Dask
  - Profiling and benchmarking

## 8. Recommended Reading and Resources

### From Your Personal Collection

1. **Python for Data Analysis**
   - *Author*: Wes McKinney
   - *Location*: `My books/Python-for-Data-Analysis.pdf`
   - *Key Chapters for This Week*:
     - Chapter 5: Getting Started with pandas
     - Chapter 8: Data Wrangling: Join, Combine, and Reshape
     - Chapter 10: Data Aggregation and Group Operations
     - Chapter 11: Time Series
   - *Why It's Useful*: Written by the creator of Pandas, this book provides authoritative coverage of data manipulation and analysis.

2. **Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow**
   - *Author*: Aurlien Gron
   - *Location*: `My books/Hands-On_Machine_Learning_with_Scikit-Learn_Keras_and_Tensorflow_-_Aurelien_Geron.pdf`
   - *Key Chapters for This Week*:
     - Chapter 2: End-to-End Machine Learning Project
     - Chapter 3: Classification
   - *Why It's Useful*: Excellent for understanding the practical application of data preprocessing and feature engineering.

3. **Deep Learning**
   - *Authors*: Ian Goodfellow, Yoshua Bengio, Aaron Courville
   - *Location*: `My books/Deep Learning by Ian Goodfellow, Yoshua Bengio, Aaron Courville.pdf`
   - *Key Chapters for This Week*:
     - Chapter 2: Linear Algebra
     - Chapter 5: Machine Learning Basics
   - *Why It's Useful*: Provides the mathematical foundation for understanding data transformations and model training.

4. **Building Machine Learning Powered Applications**
   - *Author*: Emmanuel Ameisen
   - *Location*: `My books/building-machine-learning-powered-applications-going-from-idea-to-product.pdf`
   - *Key Chapters for This Week*:
     - Chapter 3: Data Collection and Management
     - Chapter 4: From Model to Production
   - *Why It's Useful*: Covers practical aspects of working with data in real-world applications.

### Additional Recommended Textbooks

1. **Python for Data Analysis, 2nd Edition**
   - *Author*: Wes McKinney (Creator of Pandas)
   - *Coverage*: Comprehensive guide to data manipulation with Pandas
   - *Relevant Chapters*:
     - Chapter 5: Getting Started with pandas
     - Chapter 8: Data Wrangling: Join, Combine, and Reshape
     - Chapter 10: Data Aggregation and Group Operations
     - Chapter 11: Time Series

2. **Python Data Science Handbook**
   - *Author*: Jake VanderPlas
   - *Coverage*: Essential tools for working with data in Python
   - *Relevant Chapters*:
     - Chapter 3: Data Manipulation with Pandas
     - Chapter 4: Visualization with Matplotlib
     - Chapter 5: Machine Learning

3. **Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow**
   - *Author*: Aurlien Gron
   - *Coverage*: Practical machine learning with Python
   - *Relevant Chapters*:
     - Chapter 2: End-to-End Machine Learning Project
     - Chapter 3: Classification
     - Chapter 7: Ensemble Learning and Random Forests

### Time Series Analysis

1. **Time Series Analysis and Its Applications: With R Examples**
   - *Authors*: Robert H. Shumway, David S. Stoffer
   - *Coverage*: Fundamental time series concepts with practical examples
   - *Key Topics*:
     - Time series regression and exploratory data analysis
     - ARIMA models
     - Spectral analysis and filtering

2. **Forecasting: Principles and Practice**
   - *Authors*: Rob J Hyndman, George Athanasopoulos
   - *Available*: [Online for free](https://otexts.com/fpp3/)
   - *Coverage*: Practical time series forecasting
   - *Key Features*:
     - R examples with Python equivalents available
     - Focus on practical applications
     - Comprehensive coverage of forecasting methods

### Data Visualization

1. **The Visual Display of Quantitative Information**
   - *Author*: Edward R. Tufte
   - *Coverage*: Classic text on data visualization
   - *Key Concepts*:
     - Graphical integrity
     - Data-ink ratio
     - Chartjunk and data density

2. **Storytelling with Data**
   - *Author*: Cole Nussbaumer Knaflic
   - *Coverage*: Effective data communication
   - *Key Topics*:
     - Choosing the right visualization
     - Eliminating clutter
     - Focusing attention on important information

### Advanced Topics

1. **Python for Finance**
   - *Author*: Yves Hilpisch
   - *Coverage*: Financial applications of Python
   - *Relevant Chapters*:
     - Chapter 5: Data Visualization
     - Chapter 6: Financial Time Series
     - Chapter 7: Input/Output Operations

2. **Deep Learning with Python**
   - *Author*: Franois Chollet
   - *Coverage*: Deep learning with Keras and TensorFlow
   - *Relevant Chapters*:
     - Chapter 3: Getting Started with Neural Networks
     - Chapter 6: Deep Learning for Text and Sequences
     - Chapter 9: Generative Deep Learning

### Online Resources

1. **Pandas Documentation**
   - *URL*: [pandas.pydata.org](https://pandas.pydata.org/)
   - *Content*: Official documentation with tutorials and API reference

2. **Matplotlib Gallery**
   - *URL*: [matplotlib.org/stable/gallery/](https://matplotlib.org/stable/gallery/)
   - *Content*: Example plots with source code

3. **Towards Data Science (Medium)**
   - *URL*: [towardsdatascience.com](https://towardsdatascience.com/)
   - *Content*: Articles on data science and visualization

4. **Real Python**
   - *URL*: [realpython.com](https://realpython.com/)
   - *Content*: Tutorials on Python programming and data science

### Academic Papers

1. **"Pandas: Foundational Python Library for Data Analysis and Statistics"**
   - *Authors*: The Pandas Development Team
   - *Published*: 2020
   - *DOI*: 10.5281/zenodo.3509134

2. **"Matplotlib: A 2D Graphics Environment"**
   - *Authors*: J.D. Hunter
   - *Published*: Computing in Science & Engineering, 2007
   - *DOI*: 10.1109/MCSE.2007.55

### Reading Strategy

1. **For Beginners**
   - Start with "Python for Data Analysis" for Pandas fundamentals
   - Supplement with online documentation and tutorials
   - Practice with real-world datasets

2. **For Intermediate Users**
   - Focus on "Time Series Analysis and Its Applications"
   - Explore advanced Pandas features
   - Study visualization best practices

3. **For Advanced Users**
   - Delve into academic papers
   - Contribute to open-source projects
   - Stay updated with the latest research

### Additional Learning Paths

1. **DataCamp**
   - Pandas Foundations
   - Manipulating Time Series Data in Python
   - Data Visualization with Python

2. **Coursera**
   - Applied Data Science with Python (University of Michigan)
   - Python Data Analysis (Rice University)

3. **edX**
   - Data Science and Machine Learning Essentials (Microsoft)
   - Python for Data Science (UC San Diego)

## 5. Project Implementation

### 5.1 Project Overview
- Problem statement
- Dataset description
- Evaluation criteria
- Deliverables

### 5.2 Implementation Guidelines
- Data loading and cleaning
- Exploratory data analysis
- Feature engineering
- Model development

### 5.3 Best Practices
- Code organization
- Documentation
- Version control
- Reproducibility

## Exercises and Practice
- Hands-on exercises for each section
- Solutions and explanations
- Additional practice problems

## Additional Resources
- Recommended readings
- Online tutorials
- Documentation links
- Community resources

## Glossary
- Key terms and definitions
- Common functions and methods
- Best practices

## References
- Academic papers
- Blog posts
- Documentation
- Research articles
